{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ci√™ncia dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Rafael Henrique Belini Zanfolin\n",
    "\n",
    "Nome: _____Estou fazendo sozinho______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aten√ß√£o: Ser√£o permitidos grupos de tr√™s pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisar√£o fazer um question√°rio de avalia√ß√£o de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diret√≥rio\n",
      "C:\\Git\\Estudo_CDados\\Atividade_1\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diret√≥rio')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e n√£o relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.ExcelFile('Spacex.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificador autom√°tico de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fa√ßa aqui uma descri√ß√£o do seu produto e o que considerou como relevante ou n√£o relevante na classifica√ß√£o dos tweets.\n",
    "\n",
    "ESCREVA AQUI..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Treinamento\n",
       "rt @geralgeografia: superf√≠cie de alguns sat√©lites naturais do sistema solar! sigam @decifrandoastronomia üî≠üåë #astronomy #astronomia #sistem‚Ä¶                                                                                                                                                                                                 0\n",
       "rt @rocha_humberto: nasa contrata spacex para miss√£o a asteroide que poderia tornar todo mundo bilion√°rio https://t.co/yqxl0nmqm2                                                                                                                                                                                                            0\n",
       "a empresa de #elonmusk recusou mudar a rota do seu #sat√©lite. a altera√ß√£o de traject√≥ria foi feita com sucesso pela @esa esa, que teme aumento de ...https://t.co/fmux6rv2x2                                                                                                                                                                 0\n",
       "@pnolasco2000 @patocorporation se ainda tivesse a base de alc√¢ntara pra oferecer pra spacex ao inv√©s de ter dado de presente pro crush dele...                                                                                                                                                                                               0\n",
       "nunca vou me cansar de ver isso https://t.co/f7jfoculu6                                                                                                                                                                                                                                                                                      0\n",
       "                                                                                                                                                                                                                                                                                                                                            ..\n",
       "para musk, faculdade √© para ‚Äúdivers√£o‚Äù e n√£o para ‚Äúaprender‚Äù\\n\\no empres√°rio elon musk, que atualmente comanda a montadora de carros el√©tricos tesla e a companhia de explora√ß√£o espacial spacex, afirmou em um evento que a faculdade n√£o √© o ambiente ideal pa‚Ä¶ https://t.co/es7ytshat6                                                    0\n",
       "@nasa @spacex @space_station @iss_research @astro_jessica #eusalveioplaneta\\nlancar foguetes com algivas nucleares volta contra o que explode combustiva e queima e o elemento existente na terra que inclusive nos protege oxigenio foguetes lancam oxigenio no asteroide. estude esta ideia que pode salvar o planeta #eusalveioplaneta    0\n",
       "@henrypster old mo.... viajar com a spacex                                                                                                                                                                                                                                                                                                   0\n",
       "rt @tec_mundo: ceo da tesla e da spacex afirma em evento que momento da vida √© apenas para 'divers√£o'! https://t.co/tjfrvizwxw #tecmundo                                                                                                                                                                                                     0\n",
       "que lindo es esto, no? \\n@spacex @space_station #dragon https://t.co/oozv6rooat                                                                                                                                                                                                                                                              0\n",
       "Name: qual, Length: 157, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treinamento = pd.read_excel(dados, 'Treinamento')\n",
    "treinamento_reindexado = treinamento.set_index(\"Treinamento\")\n",
    "treinamento_reindexado.qual[treinamento_reindexado.qual == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = pd.read_excel(dados, 'Teste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevante = treinamento[treinamento.qual == 1]\n",
    "#relevante_treinamento = relevante.Treinamento\n",
    "relevante_treinamento = relevante.Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevante = treinamento[treinamento.qual ==1]\n",
    "irrelevante_treinamento = irrelevante.Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     @jairbolsonaro @realdonaldtrump sr. presidente...\n",
       "2     elon musk: os sat√©lites de internet starlink d...\n",
       "7     nasa e spacex v√£o trabalhar em conjunto para a...\n",
       "8     rt @ned35: spacex anuncia parceria para enviar...\n",
       "12    asteroide feito de ouro √© ‚Äúcobi√ßado‚Äù por nasa ...\n",
       "Name: Treinamento, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevante_treinamento.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevante = treinamento[treinamento.qual ==0]\n",
    "irrelevante_treinamento = irrelevante.Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    rt @geralgeografia: superf√≠cie de alguns sat√©l...\n",
       "3    rt @rocha_humberto: nasa contrata spacex para ...\n",
       "4    a empresa de #elonmusk recusou mudar a rota do...\n",
       "5    @pnolasco2000 @patocorporation se ainda tivess...\n",
       "6    nunca vou me cansar de ver isso https://t.co/f...\n",
       "Name: Treinamento, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irrelevante_treinamento.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_relevante = \"\"\n",
    "for plvrs in relevante_treinamento:\n",
    "    string_relevante = string_relevante + plvrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string_relevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_irrelevante = \"\"\n",
    "for plvras in irrelevante_treinamento:\n",
    "    string_irrelevante = string_irrelevante + plvras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string_irrelevante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M√©todo de limpeza do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def cleanup(text):\n",
    "    texto_sem_rt = text.replace(\"rt\", \"\")\n",
    "    \"\"\"\n",
    "        Fun√ß√£o de limpeza muito simples que troca alguns sinais b√°sicos por espa√ßos\n",
    "    \"\"\"\n",
    "    punctuation = \"[!\\-:?;.@//]\" # Note que os sinais [] s√£o delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, '', texto_sem_rt)\n",
    "    return text_subbed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevante_limpo = cleanup(string_relevante)\n",
    "#relevante_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevante_limpo = cleanup(string_irrelevante)\n",
    "#irrelevante_limpo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela de Frequ√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevante_splitado = pd.Series(relevante_limpo.split())\n",
    "relevante_splitado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevante_splitado = pd.Series(irrelevante_limpo.split())\n",
    "irrelevante_splitado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_relevante = relevante_limpo.split()\n",
    "split_irrelevante = irrelevante_limpo.split()\n",
    "\n",
    "while len(split_irrelevante) < len(split_relevante):\n",
    "    split_irrelevante.append(0)\n",
    "    \n",
    "    \n",
    "dicionario = {'relevante' : split_relevante,\n",
    "              'irrelevante' : split_irrelevante\n",
    "            }\n",
    "\n",
    "DataFrame = pd.DataFrame(dicionario)\n",
    "#DataFrame.groupby('relevante')['irrelevante'].value_counts(True)\n",
    "DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##teste\n",
    "#limpo_teste = []\n",
    "#for pvra in inespressivo_limpo.split():\n",
    "    #nova_pvra = pvra.replace(\" \",\"\")\n",
    "    #limpo_teste.append(nova_pvra)\n",
    "    \n",
    "#print(pd.Series(limpo_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqabs_relevante = relevante_splitado.value_counts()\n",
    "freqabs_relevante.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = relevante_limpo + irrelevante_limpo\n",
    "total_series = pd.Series(total.split())\n",
    "total_series.isin(relevante_splitado).value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqabs_irrelevante = irrelevante_splitado.value_counts()\n",
    "freqabs_irrelevante.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqrel_relevante = relevante_splitado.value_counts(True)\n",
    "freqrel_relevante.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqrel_irrelevante = irrelevante_splitado.value_counts(True) \n",
    "freqrel_irrelevante.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_palavras = relevante_limpo + irrelevante_limpo\n",
    "total_palavras_splitado = pd.Series(total_palavras.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_total_abs = total_palavras_splitado.value_counts()\n",
    "freq_total_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_total_rel = total_palavras_splitado.value_counts(True)\n",
    "freq_total_rel.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby([relevante_splitado,irrelevante_splitado]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# palv_relevante = []\n",
    "# for vle in relevante_limpo:\n",
    "#     palv_relevante.append(vle)\n",
    "\n",
    "# plvras_relevantes = len(palv_relevante)\n",
    "# plvras_relevantes_setado = len(list(set(palv_relevante)))\n",
    "\n",
    "# palv_irrelevante = []\n",
    "# for vl in irrelevante_splitado:\n",
    "#     palv_irrelevante.append(vl)\n",
    "\n",
    "# plvras_irrelevante = len(palv_irrelevante)\n",
    "# plvras_irrelevante_setado = len(list(set(palv_irrelevante)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_relevantes = len(relevante_splitado)\n",
    "palavras_relevantes_setado = len(list(set(relevante_splitado)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_irrelevantes = len(irrelevante_splitado)\n",
    "palavras_irrelevantes_setado = len(list(set(irrelevante_splitado)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Este √© o n√∫mero de palavras expressivas relevantes com repeti√ß√µes : {0}\".format(palavras_relevantes))\n",
    "print(\"Este √© o n√∫mero de palavras expressivas relevantes que n√£o s√£o repetidas: {0}\".format(palavras_relevantes_setado))\n",
    "print(\"Este √© o n√∫mero de palavras inexpressivas relevantes com repeti√ß√µes: {0}\".format(palavras_irrelevantes))\n",
    "print(\"Este √© o n√∫mero de palavras inespressivas relevantes que n√£o s√£o repetidas: {0}\".format(palavras_irrelevantes_setado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_palavras = palavras_relevantes + palavras_irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Este √© o n√∫mero total de palavras que foram analisadas : {0}\".format(total_palavras))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_relevantes = palavras_relevantes / total_palavras\n",
    "prob_relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_irrelevantes = palavras_irrelevantes / total_palavras\n",
    "prob_irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prob_relevantes + prob_irrelevantes == 1:\n",
    "    print(\"Correto, a soma √© igual a 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_relevante_intersec_relevantes = prob_relevantes * prob_relevantes\n",
    "prob_relevante_intersec_relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_relevante_intersec_irrelevante = prob_irrelevantes * prob_irrelevantes\n",
    "prob_relevante_intersec_irrelevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_relativa_relevante = relevante_splitado.value_counts(True)\n",
    "todas_palavras_relevantes = list(freq_relativa_relevante.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_relativa_irrelevante = irrelevante_splitado.value_counts(True)\n",
    "todas_palavras_irrelevantes = list(freq_relativa_irrelevante.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_supermercado = \"arroz batata trakinas biscoito cereja farinha\"\n",
    "comida_saudavel = \"banana arroz feij√£o batata ma√ß√£ pera batata\"\n",
    "comida_nsaudavel = \"tequila vodka trakinas biscoito bolacha ac√∫car\"\n",
    "\n",
    "cat_1 = pd.Series(comida_saudavel.split())\n",
    "cat_2 = pd.Series(comida_nsaudavel.split())\n",
    "judge = pd.Series(lista_supermercado.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_judge = judge.value_counts()\n",
    "counting_judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_cat_1 = cat_1.value_counts()\n",
    "counting_cat_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_cat_2 = cat_2.value_counts()\n",
    "counting_cat_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply_series = counting_cat_2.multiply(counting_judge)\n",
    "fill_nan = multiply_series.fillna(0)\n",
    "fill_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_1 = fill_nan.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words_cat_1 = counting_cat_1.sum()\n",
    "n_words_cat_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = list(cat_1)\n",
    "list_2 = list(cat_2)\n",
    "list_3 = list(judge)\n",
    "\n",
    "setado = set(list_1 + list_3)\n",
    "lgt_setado = len(setado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = add_1 / (len(list_1) + lgt_setado )\n",
    "\n",
    "multiply = 1\n",
    "for values in result:\n",
    "    multiply = multiply * values\n",
    "    \n",
    "multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#judge.groupby(cat_1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothin(catg_1,catg_2,tseries):    \n",
    "    counting_catg1 = catg_1.value_counts()\n",
    "    counting_catg2 = catg_2.value_counts()\n",
    "    counting_tseries = tseries.value_counts()\n",
    "    \n",
    "    multiply_catg1_tseries = counting_catg1.multiply(counting_tseries)\n",
    "    multiply_catg2_tseries = counting_catg2.multiply(counting_tseries)\n",
    "    \n",
    "    list_1 = list(catg_1)\n",
    "    #print(list_1)\n",
    "    list_2 = list(catg_2)\n",
    "    #print(list_2)\n",
    "    list_3 = list(tseries)\n",
    "    #print(list_3)\n",
    "    \n",
    "    n_words_catg1 = counting_catg1.sum()\n",
    "    n_words_catg2 = counting_catg2.sum()\n",
    "    \n",
    "    seting_catg1_tseries = set(list_1 + list_3)\n",
    "    seting_catg2_tseries = set(list_2 + list_3)\n",
    "    \n",
    "    fill_nan_catg1 = multiply_catg1_tseries.fillna(0)\n",
    "    fill_nan_catg2 = multiply_catg2_tseries.fillna(0)\n",
    "    \n",
    "    add_1_catg1 = fill_nan_catg1.add(1)\n",
    "    add_1_catg2 = fill_nan_catg2.add(1)\n",
    "    \n",
    "    result_catg1 = add_1_catg1 / (len(list_1) + len(seting_catg1_tseries))\n",
    "    result_catg2 = add_1_catg2 / (len(list_1) + len(seting_catg2_tseries))\n",
    "    \n",
    "    log_catg1 = []\n",
    "    log_catg2 = []\n",
    "    for values in result_catg1:\n",
    "        log_prob = math.log(values) / 10\n",
    "        log_catg1.append(log_prob)\n",
    "    \n",
    "    for vles in result_catg2:\n",
    "        log_prob = math.log(vles) / 10\n",
    "        log_catg2.append(log_prob)\n",
    "        \n",
    "    prob_catg1 = 1\n",
    "    for valores_1 in log_catg1:\n",
    "        prob_catg1 = prob_catg1 * valores_1\n",
    "    \n",
    "    prob_catg2 = 1\n",
    "    for valores_2 in log_catg2:\n",
    "        prob_catg2 = prob_catg2 * valores_2\n",
    "        \n",
    "    prob_teste = 1\n",
    "    for va in result_catg1:\n",
    "        prob_teste = prob_teste * va\n",
    "        prob_teste = prob_teste / 5\n",
    "        \n",
    "    if prob_catg1 > prob_catg2:\n",
    "        print(\"√â mais prov√°vel que fa√ßa parte da categoria 1!\")\n",
    "    else:\n",
    "        print(\"√â mais prov√°vel que fa√ßa parte da categoria 2!\")\n",
    "    \n",
    "    return math.trunc(prob_catg1), math.trunc(prob_catg2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cat_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-b8b50955bc07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlaplace_smoothin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcat_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mjudge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cat_1' is not defined"
     ]
    }
   ],
   "source": [
    "print(laplace_smoothin(cat_1,cat_2,judge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. \"\n",
    "print(len(string))\n",
    "abd = pd.Series(string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(laplace_smoothin(relevante_splitado, irrelevante_splitado, abd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersec = pd.Series(list(set(relevante_splitado) & set(irrelevante_splitado)))\n",
    "intersec.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total_palavras_splitado.count()\n",
    "A = relevante_splitado.count()\n",
    "B = irrelevante_splitado.count()\n",
    "\n",
    "prob_A = A / total\n",
    "prob_B = B / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_list = []\n",
    "for palavras in list(relevante_splitado):\n",
    "    if palavras in list(irrelevante_splitado):\n",
    "        intersection_list.append(palavras)\n",
    "        \n",
    "len(intersection_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treinamento_reindexado = treinamento.set_index(\"Treinamento\")\n",
    "treinamento_reindexado.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aperfei√ßoamento:\n",
    "\n",
    "Os trabalhos v√£o evoluir em conceito dependendo da quantidade de itens avan√ßados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separa√ß√£o de espa√ßos entre palavras e emojis ou entre emojis e emojis\n",
    "* Propor outras limpezas e transforma√ß√µes que n√£o afetem a qualidade da informa√ß√£o ou classifica√ß√£o\n",
    "* Criar categorias intermedi√°rias de relev√¢ncia baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que n√£o posso usar o pr√≥prio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cen√°rios para Na√Øve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indica√ß√µes concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que periodicamente realiza an√°lise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Refer√™ncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
